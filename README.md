# â­ Routing Foundation Model (RFM)
### *A Unified Neural Optimization Framework for Large-Scale Routing and MILPs*

<p align="left">
  <img src="https://img.shields.io/badge/status-in%20progress-yellow?style=flat-square" />
  <img src="https://img.shields.io/badge/python-3.10+-blue?style=flat-square" />
  <img src="https://img.shields.io/badge/pytorch-2.x-red?style=flat-square" />
  <img src="https://img.shields.io/badge/license-MIT-purple?style=flat-square" />
</p>

The **Routing Foundation Model (RFM)** is a neural surrogate optimization architecture designed to solve large-scale **routing** and **mixed-integer linear programs (MILPs)** at *neural network inference speed*.  
RFM builds on the insight that **transformers behave like unrolled optimizers**, enabling them to approximate MILP reasoning when equipped with the right inductive biases.

RFM combines:
- ğŸ§  **MILP-aware attention (Aáµ€v dual correction)**
- ğŸ”§ **constraint-specialized Mixture-of-Experts**
- ğŸ”„ **latent gradient refinement**
- ğŸ§® **soft integer relaxations**
- âœ¨ **iterative transformer-like updates**

RFM is intended for Amazon-scale middle-mile routing, VRP variants, supply chain optimization, and general combinatorial optimization tasks.

---

## ğŸš€ Features

### ğŸ”· Transformer as an Optimizer  
Attention â‰ˆ proximal update, residuals â‰ˆ gradient descent, dual terms â‰ˆ feasibility correction.

### ğŸ”· MILP-Aware Attention  
Injects feasibility structure directly into logits:
```
L = QKáµ€/âˆšd - Î³ * (Aáµ€v)
```

### ğŸ”· Mixture-of-Experts for Constraints  
Experts specialize to:
- flow conservation  
- capacity  
- activation/binary coupling  
- SLA/time-window constraints  

### ğŸ”· Latent Optimization Loop  
Gradient-like refinement mimics interior-point / dual ascent behavior.

### ğŸ”· Extensible  
Replace encoder with GNN/Graphormer/Mamba, add diffusion priors, or warm-start Gurobi.

---

## ğŸ—ï¸ Repository Structure

```
rfm/
â”‚
â”œâ”€â”€ rfm/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ milp_transformer.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ milp_attention.py
â”‚   â”‚   â””â”€â”€ feedforward.py
â”‚   â”‚
â”‚   â”œâ”€â”€ solvers/
â”‚   â”‚   â”œâ”€â”€ constraint_experts.py
â”‚   â”‚   â””â”€â”€ refinement.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ milp.py
â”‚   â”‚   â””â”€â”€ graph.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ synthetic_50_nodes.py
â”‚
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ RFM_monograph.tex
â”‚   â””â”€â”€ references.bib
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš¡ Quickstart

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/ritwikareddykancharla/rfm.git
cd rfm
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -e .
```

### 3ï¸âƒ£ Run a synthetic routing problem
```bash
python experiments/synthetic_50_nodes.py
```

---

## ğŸ”¬ How RFM Works

### 1. Soft binary relaxation
```python
x = sigmoid(logits / tau)
```

### 2. Constraint violations  
```python
v = relu(A @ x - b)
```

### 3. Dual-inspired correction  
```python
h = A.T @ v
```

### 4. MILP-aware attention  
```python
L = QK^T / sqrt(d) - Î³ * h
Î± = softmax(L)
```

### 5. Constraint experts refine feasibility  
```python
Î¦ = MoE(v)
```

### 6. Latent gradient-style refinement  
```python
x = x - Î· * âˆ‡(cáµ€x + Î¦)
```

---

## ğŸ“š Citation

```bibtex
@misc{kancharla2025rfm,
  title={Routing Foundation Model (RFM): A Unified Neural Optimization Framework for Large-Scale Routing and MILPs},
  author={Kancharla, Ritwika},
  year={2025},
  archivePrefix={arXiv},
}
```

---

## ğŸ“¬ Contact  
**Ritwika Kancharla**  
ğŸ“§ ritwikareddykancharla@gmail.com
